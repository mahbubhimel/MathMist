{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13260558,"sourceType":"datasetVersion","datasetId":8402995}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:06:20.055498Z","iopub.execute_input":"2025-10-04T14:06:20.055936Z","iopub.status.idle":"2025-10-04T14:06:20.383461Z","shell.execute_reply.started":"2025-10-04T14:06:20.055906Z","shell.execute_reply":"2025-10-04T14:06:20.382449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nRefactored Gemini Language Classification Tool (single-file)\n- Uses a single API key path / env variable (GEMINI_API_KEY)\n- More comprehensive, strict, flat-JSON prompt (no arrays/nested objects)\n- Parses semicolon-separated phrase fields returned by the model\n- Preserves API key rotation + background worker behavior\n- NOW INCLUDES CHINESE LANGUAGE DETECTION AND COMPREHENSIVE LANGUAGE ANALYSIS\n- ENHANCED: Better markdown and LLM output pattern removal\n\nUsage: edit paths in `main()` and run.\n\"\"\"\n\nimport os\nimport re\nimport time\nimport json\nimport queue\nimport logging\nimport threading\nimport traceback\nfrom tqdm import tqdm\nfrom collections import deque, defaultdict\nfrom datetime import datetime\nfrom typing import Optional, Dict, Any, List, Tuple\nfrom string import Template\n\ntry:\n    from google import genai  # type: ignore\nexcept Exception:\n    genai = None\n\n# ---------------------------\n# Defaults / Configuration\n# ---------------------------\nDEFAULT_MODEL = \"gemini-2.5-flash-lite\"\nDEFAULT_RATE_LIMIT = 5.0\nDEFAULT_CALLS_PER_DAY = 1000\n\n# ---------------------------\n# Logging Configuration\n# ---------------------------\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[logging.FileHandler('language_classification_gemini.log'), logging.StreamHandler()],\n)\n\n# ---------------------------\n# Load API Keys (single path)\n# ---------------------------\n\ndef load_api_keys(key_path: Optional[str] = None) -> List[str]:\n    \"\"\"Load a single API key from a file or the GEMINI_API_KEY environment variable.\"\"\"\n    if key_path:\n        if os.path.exists(key_path):\n            try:\n                with open(key_path, 'r', encoding='utf-8') as fh:\n                    raw = fh.read().strip()\n                if not raw:\n                    raise ValueError(f\"Key file {key_path} is empty\")\n                keys = [k.strip() for k in re.split(r'[\\n,]+', raw) if k.strip()]\n                if not keys:\n                    raise ValueError(f\"No usable keys found in {key_path}\")\n                if len(keys) > 1:\n                    logging.warning(f\"Multiple keys found in {key_path}; using the first one\")\n                logging.info(f\"Loaded API key from {key_path}\")\n                return [keys[0]]\n            except Exception as e:\n                logging.warning(f\"Failed to load key from {key_path}: {e}\")\n        else:\n            logging.warning(f\"Provided key path does not exist: {key_path}\")\n\n    env_key = os.environ.get('GEMINI_API_KEY', '').strip()\n    if env_key:\n        logging.info(\"Loaded API key from GEMINI_API_KEY environment variable\")\n        return [env_key]\n\n    raise ValueError(\"No API key found. Provide a key file path or set GEMINI_API_KEY\")\n\n# ---------------------------\n# Gemini API Manager\n# ---------------------------\nclass GeminiLanguageApiManager:\n    \"\"\"Manage multiple (or single) Gemini API keys with rotation and a simple rate limiter.\"\"\"\n\n    def __init__(self, api_keys: List[str], calls_per_day: int = DEFAULT_CALLS_PER_DAY, rate_limit_delay: float = DEFAULT_RATE_LIMIT):\n        if not api_keys:\n            raise ValueError(\"api_keys must contain at least one key\")\n\n        self.api_keys = deque(api_keys)\n        self.calls_per_day = calls_per_day\n        self.rate_limit_delay = rate_limit_delay\n\n        self.usage_count: Dict[str, int] = {k: 0 for k in api_keys}\n        self.current_key = self.api_keys[0]\n\n        if genai is None:\n            logging.warning(\"google.genai package not available. API calls will fail until genai is installed.\")\n            self.client = None\n        else:\n            self.client = genai.Client(api_key=self.current_key)\n\n        self.lock = threading.Lock()\n\n        self.call_queue: queue.Queue = queue.Queue()\n        self.worker_thread = threading.Thread(target=self._process_queue, name=\"GeminiWorker\")\n        self.worker_thread.daemon = True\n        self.worker_thread.start()\n\n        logging.info(f\"Language API Manager initialized with {len(api_keys)} key(s)\")\n\n    def _rotate_key(self) -> None:\n        with self.lock:\n            self.api_keys.rotate(1)\n            self.current_key = self.api_keys[0]\n            if genai is not None:\n                self.client = genai.Client(api_key=self.current_key)\n            usage = self.usage_count.get(self.current_key, 0)\n        logging.info(f\"Rotated to new API key (usage: {usage})\")\n\n    def _find_available_key(self) -> bool:\n        with self.lock:\n            if self.usage_count.get(self.current_key, 0) < self.calls_per_day:\n                return True\n\n        initial = self.current_key\n        for _ in range(len(self.api_keys)):\n            self._rotate_key()\n            with self.lock:\n                if self.usage_count.get(self.current_key, 0) < self.calls_per_day:\n                    return True\n            if self.current_key == initial:\n                return False\n        return False\n\n    def _process_queue(self) -> None:\n        while True:\n            try:\n                args, kwargs, result_queue = self.call_queue.get()\n\n                if not self._find_available_key():\n                    try:\n                        result_queue.put({\"error\": \"All API keys have reached their daily limit\"})\n                    except Exception:\n                        logging.exception(\"Failed to notify caller about exhausted keys\")\n                    self.call_queue.task_done()\n                    time.sleep(10)\n                    continue\n\n                try:\n                    if self.client is None:\n                        raise RuntimeError(\"genai.Client not initialized (missing genai package or client)\")\n\n                    response = None\n                    try:\n                        response = self.client.models.generate_content(*args, **kwargs)\n                    except Exception as api_exc:\n                        msg = str(api_exc).lower()\n                        if 'quota' in msg or 'rate limit' in msg or 'quota_exceeded' in msg:\n                            with self.lock:\n                                self.usage_count[self.current_key] = self.calls_per_day\n                            logging.warning(f\"API key reached quota/rate-limit: {api_exc}\")\n                            result_queue.put({\"error\": f\"Rate limit/quota: {api_exc}\"})\n                        else:\n                            logging.error(f\"API call error: {api_exc}\")\n                            result_queue.put({\"error\": str(api_exc)})\n                        response = None\n\n                    if response is not None:\n                        result_queue.put({\"response\": response})\n                        with self.lock:\n                            self.usage_count[self.current_key] = self.usage_count.get(self.current_key, 0) + 1\n\n                except Exception as e:\n                    logging.error(f\"Unexpected API invocation error: {e}\\n{traceback.format_exc()}\")\n                    try:\n                        result_queue.put({\"error\": str(e)})\n                    except Exception:\n                        logging.exception(\"Failed to send error to result queue\")\n\n                time.sleep(self.rate_limit_delay)\n                self.call_queue.task_done()\n\n            except Exception as e:\n                logging.error(f\"Queue processing error: {e}\\n{traceback.format_exc()}\")\n                time.sleep(1)\n                continue\n\n    def generate_content(self, *args, timeout: float = 60.0, **kwargs) -> Any:\n        result_queue: queue.Queue = queue.Queue()\n        self.call_queue.put((args, kwargs, result_queue))\n        try:\n            result = result_queue.get(timeout=timeout)\n        except queue.Empty:\n            raise TimeoutError(\"Timed out waiting for API worker result.\")\n\n        if \"error\" in result:\n            raise Exception(result[\"error\"])\n        return result[\"response\"]\n\n    def get_usage_stats(self) -> Dict[str, Any]:\n        with self.lock:\n            per_key = dict(self.usage_count)\n        total_used = sum(per_key.values())\n        total_available = len(self.api_keys) * self.calls_per_day\n        return {\n            \"per_key\": per_key,\n            \"total_used\": total_used,\n            \"total_available\": total_available,\n            \"percent_used\": (total_used / total_available) * 100 if total_available > 0 else 0,\n        }\n\n# ---------------------------\n# Text Cleaning & Parsing\n# ---------------------------\nCOT_PHRASES = [\n    \"step 1\", \"step 2\", \"step 3\", \"step 4\", \"step 5\", \"step 6\", \"step 7\", \"step 8\", \"step 9\", \"step 10\",\n    \"step-by-step\", \"step-by-step solution\", \"step by step\", \"step\",\n    \"solution\", \"final answer\", \"final\", \"proved\",\n    \"verification\", \"verify\",\n    \"problem understanding\", \"Problem Understanding\", \"PROBLEM UNDERSTANDING\", \n    \"problem statement\", \"Problem statement\", \"Problem Statement\", \"PROBLEM STATEMENT\",\n    \"mathematical analysis\", \"Mathematical Analysis\", \"MATHEMATICAL ANALYSIS\",\n    \"in bangla\", \"In Bangla\",\n    \"analysis\", \"problem\", \"understanding\", \"approach\", \"method\", \"calculation\", \"result\",\n    \"conclusion\", \"proof\", \"given\", \"find\", \"solve\", \"answer\", \"check\",\n    \"first\", \"second\", \"third\", \"next\", \"then\", \"finally\", \"lastly\",\n    \"mathematical\", \"formula\", \"equation\", \"expression\", \"value\", \"number\",\n    \"let\", \"assume\", \"suppose\", \"consider\", \"note\", \"observe\", \"recall\",\n    \"definition\", \"theorem\", \"lemma\", \"corollary\", \"proposition\",\n]\n\n\ndef clean_text_for_language_analysis(text: str, cot_phrases: Optional[List[str]] = None) -> str:\n    \"\"\"Clean text by removing math, CoT phrases, and structural elements.\"\"\"\n    if not text:\n        return \"\"\n\n    if cot_phrases is None:\n        cot_phrases = COT_PHRASES\n\n    # Remove <think> tags first\n    text = re.sub(r'<think>.*?</think>', ' ', text, flags=re.DOTALL | re.IGNORECASE)\n    \n    # Remove <final> tags\n    text = re.sub(r'<final>.*?</final>', ' ', text, flags=re.DOTALL | re.IGNORECASE)\n    \n    # Remove markdown bold/italic formatting (enhanced)\n    text = re.sub(r'\\*\\*\\*([^*]+)\\*\\*\\*', r'\\1', text)  # ***bold+italic***\n    text = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', text)      # **bold**\n    text = re.sub(r'\\*([^*]+)\\*', r'\\1', text)          # *italic*\n    text = re.sub(r'___([^_]+)___', r'\\1', text)        # ___bold+italic___\n    text = re.sub(r'__([^_]+)__', r'\\1', text)          # __bold__\n    text = re.sub(r'_([^_]+)_', r'\\1', text)            # _italic_\n    \n    # Remove strikethrough\n    text = re.sub(r'~~([^~]+)~~', r'\\1', text)          # ~~strikethrough~~\n    \n    # Remove inline code\n    text = re.sub(r'`([^`]+)`', r'\\1', text)            # `code`\n    \n    # Remove code blocks (triple backticks)\n    text = re.sub(r'```[a-z]*\\n.*?\\n```', ' ', text, flags=re.DOTALL)\n    \n    # Remove markdown headers\n    text = re.sub(r'^#{1,6}\\s+', '', text, flags=re.MULTILINE)\n    \n    # Remove markdown links [text](url)\n    text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)\n    \n    # Remove markdown images ![alt](url)\n    text = re.sub(r'!\\[([^\\]]*)\\]\\([^\\)]+\\)', '', text)\n    \n    # Remove HTML tags (sometimes used by LLMs)\n    text = re.sub(r'<[^>]+>', ' ', text)\n    \n    # Remove blockquotes\n    text = re.sub(r'^>\\s+', '', text, flags=re.MULTILINE)\n    \n    # Remove horizontal rules\n    text = re.sub(r'^[-*_]{3,}\\s*$', '', text, flags=re.MULTILINE)\n    \n    # Remove bullet points and numbered lists\n    text = re.sub(r'^\\s*[\\*\\-\\+]\\s+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^\\s*\\d+\\.\\s+', '', text, flags=re.MULTILINE)\n    \n    # Remove common section headers (case insensitive, with optional colons/parentheses)\n    section_headers = [\n        r'problem\\s+statement',\n        r'problem\\s+understanding',\n        r'mathematical\\s+analysis',\n        r'step[-\\s]by[-\\s]step\\s+solution',\n        r'verification',\n        r'final\\s+answer',\n        r'conclusion',\n        r'approach',\n        r'solution',\n    ]\n    \n    for header in section_headers:\n        pattern = r'\\b' + header + r'\\s*[:\\(\\)]?\\s*'\n        text = re.sub(pattern, ' ', text, flags=re.IGNORECASE)\n    \n    # Remove mathematical content\n    text = re.sub(r'\\$.*?\\$', ' ', text)\n    text = re.sub(r'\\\\\\[.*?\\\\\\]', ' ', text, flags=re.DOTALL)\n    text = re.sub(r'\\\\\\(.*?\\\\\\)', ' ', text)\n    text = re.sub(r'\\\\begin\\{.*?\\}.*?\\\\end\\{.*?\\}', ' ', text, flags=re.DOTALL)\n    text = re.sub(r'\\\\[a-zA-Z]+\\{[^}]*\\}', ' ', text)\n    text = re.sub(r'\\\\[a-zA-Z]+', ' ', text)\n\n    # Remove numbers and mathematical symbols\n    text = re.sub(r'[0-9০-৯]+', ' ', text)\n    text = re.sub(r'[+\\-*/=<>≤≥≠∑∏∫∂∇±×÷√∞θπ]', ' ', text)\n    text = re.sub(r'[()\\[\\]{}|]', ' ', text)\n\n    # Remove CoT phrases\n    for phrase in cot_phrases:\n        pattern = r'\\b' + re.escape(phrase) + r'[:,.]?\\s*'\n        text = re.sub(pattern, ' ', text, flags=re.IGNORECASE)\n\n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n\ndef parse_gemini_json_response(text: str) -> Optional[Dict[str, Any]]:\n    \"\"\"Attempt to extract a JSON object from a model response.\"\"\"\n    if not text:\n        return None\n    text = text.strip()\n\n    # Try to find JSON inside ``` fences\n    fence_re = re.compile(r'```(?:json)?\\s*\\n?(.*?)\\n?```', re.DOTALL | re.IGNORECASE)\n    for match in fence_re.findall(text):\n        candidate = match.strip()\n        if not candidate:\n            continue\n        try:\n            parsed = json.loads(candidate)\n            if isinstance(parsed, dict):\n                return parsed\n        except json.JSONDecodeError:\n            fixed = re.sub(r'\\\\(?![\"\\\\/bfnrtu])', r'\\\\\\\\', candidate)\n            try:\n                parsed = json.loads(fixed)\n                if isinstance(parsed, dict):\n                    return parsed\n            except json.JSONDecodeError:\n                continue\n\n    # Try raw text\n    try:\n        parsed = json.loads(text)\n        if isinstance(parsed, dict):\n            return parsed\n    except Exception:\n        pass\n\n    # Fallback: find the first balanced top-level JSON object\n    start = None\n    depth = 0\n    for i, ch in enumerate(text):\n        if ch == '{':\n            if start is None:\n                start = i\n            depth += 1\n        elif ch == '}' and start is not None:\n            depth -= 1\n            if depth == 0:\n                candidate = text[start:i+1]\n                try:\n                    parsed = json.loads(candidate)\n                    if isinstance(parsed, dict):\n                        return parsed\n                except json.JSONDecodeError:\n                    fixed = re.sub(r'\\\\(?![\"\\\\/bfnrtu])', r'\\\\\\\\', candidate)\n                    try:\n                        parsed = json.loads(fixed)\n                        if isinstance(parsed, dict):\n                            return parsed\n                    except json.JSONDecodeError:\n                        start = None\n                        depth = 0\n                        continue\n\n    return None\n\n\ndef extract_language_classification(response_text: str) -> Tuple[Optional[str], Optional[float], List[str], List[str], List[str], List[str], str]:\n    \"\"\"Extract classification fields from a parsed JSON object or via heuristics.\"\"\"\n    if not response_text:\n        return None, None, [], [], [], [], \"\"\n\n    obj = parse_gemini_json_response(response_text)\n    if obj:\n        language = None\n        conf = None\n        bangla_phrases: List[str] = []\n        english_phrases: List[str] = []\n        chinese_phrases: List[str] = []\n        excluded_cot_phrases: List[str] = []\n        mixed_languages_present = \"\"\n\n        # language detection\n        for key in (\"language\", \"Language\", \"LANGUAGE\", \"classification\", \"Classification\"):\n            if key in obj:\n                lang_val = str(obj[key]).strip().lower()\n                if lang_val in (\"bangla\", \"english\", \"chinese\", \"mixed\"):\n                    language = lang_val\n                break\n\n        # confidence\n        for key in (\"confidence\", \"conf\", \"confidence_score\"):\n            if key in obj:\n                try:\n                    conf_val = obj[key]\n                    if isinstance(conf_val, str):\n                        conf_val = conf_val.strip()\n                        conf_val = float(conf_val)\n                    else:\n                        conf_val = float(conf_val)\n\n                    if 0.0 <= conf_val <= 1.0:\n                        conf = conf_val\n                    elif 0.0 <= conf_val <= 100.0:\n                        conf = conf_val / 100.0\n                except Exception:\n                    conf = None\n                break\n\n        # Mixed languages present\n        for key in (\"mixed_languages_present\", \"mixedLanguagesPresent\", \"languages_present\"):\n            if key in obj:\n                mixed_val = obj[key]\n                if isinstance(mixed_val, str):\n                    mixed_languages_present = mixed_val.strip()\n                elif isinstance(mixed_val, list):\n                    mixed_languages_present = \";\".join([str(x).strip() for x in mixed_val if str(x).strip()])\n                break\n\n        # Phrase fields\n        def _to_list(val: Any) -> List[str]:\n            if val is None:\n                return []\n            if isinstance(val, list):\n                return [str(x).strip() for x in val if str(x).strip()]\n            if isinstance(val, str):\n                parts = [p.strip() for p in val.split(';') if p.strip()]\n                return parts\n            return [str(val).strip()]\n\n        bangla_phrases = _to_list(obj.get('bangla_phrases') or obj.get('Bangla_phrases') or obj.get('banglaPhrases'))\n        english_phrases = _to_list(obj.get('english_phrases') or obj.get('English_phrases') or obj.get('englishPhrases'))\n        chinese_phrases = _to_list(obj.get('chinese_phrases') or obj.get('Chinese_phrases') or obj.get('chinesePhrases'))\n        excluded_cot_phrases = _to_list(obj.get('excluded_cot_phrases') or obj.get('excludedCOT') or obj.get('excluded_cot'))\n\n        # Filter out CoT phrases\n        def normalize_phrase(s: str) -> str:\n            s = s or ''\n            s = s.lower()\n            s = re.sub(r'[^a-z0-9\\s]', ' ', s)\n            s = re.sub(r'[\\s\\-_]+', ' ', s).strip()\n            return s\n\n        cot_norms = {normalize_phrase(p) for p in COT_PHRASES}\n\n        def filter_out_cot(lst: List[str]) -> List[str]:\n            out: List[str] = []\n            for ph in lst:\n                if not ph:\n                    continue\n                if normalize_phrase(ph) in cot_norms:\n                    continue\n                out.append(ph)\n            return out\n\n        bangla_phrases = filter_out_cot(bangla_phrases)\n        english_phrases = filter_out_cot(english_phrases)\n        chinese_phrases = filter_out_cot(chinese_phrases)\n        excluded_cot_phrases = filter_out_cot(excluded_cot_phrases)\n\n        if language:\n            return language, conf, bangla_phrases, english_phrases, chinese_phrases, excluded_cot_phrases, mixed_languages_present\n\n    # Fallback pattern matching\n    patterns = [\n        r'Classification:\\s*(bangla|english|chinese|mixed)',\n        r'Language:\\s*(bangla|english|chinese|mixed)',\n        r'Result:\\s*(bangla|english|chinese|mixed)',\n        r'(bangla|english|chinese|mixed)\\s*\\(confidence:\\s*([\\d.]+)\\)',\n    ]\n\n    for pattern in patterns:\n        match = re.search(pattern, response_text, re.IGNORECASE)\n        if match:\n            lang = match.group(1).lower()\n            conf = None\n            if len(match.groups()) > 1:\n                try:\n                    conf = float(match.group(2))\n                    if conf > 1.0:\n                        conf = conf / 100.0\n                except Exception:\n                    pass\n            return lang, conf, [], [], [], [], \"\"\n\n    ru = response_text.upper()\n    if \"CHINESE\" in ru and \"BANGLA\" not in ru and \"ENGLISH\" not in ru:\n        return \"chinese\", None, [], [], [], [], \"\"\n    if \"BANGLA\" in ru and \"ENGLISH\" not in ru and \"CHINESE\" not in ru:\n        return \"bangla\", None, [], [], [], [], \"\"\n    if \"ENGLISH\" in ru and \"BANGLA\" not in ru and \"CHINESE\" not in ru:\n        return \"english\", None, [], [], [], [], \"\"\n    if \"MIXED\" in ru:\n        return \"mixed\", None, [], [], [], [], \"\"\n\n    return None, None, [], [], [], [], \"\"\n\n\ndef get_heuristic_classification(text: str, cot_phrases: Optional[List[str]] = None) -> str:\n    \"\"\"Heuristic classification with Chinese detection.\"\"\"\n    try:\n        cleaned = clean_text_for_language_analysis(text, cot_phrases)\n        has_bangla = bool(re.search(r'[\\u0980-\\u09FF]', cleaned))\n        has_english = bool(re.search(r'[a-zA-Z]{2,}', cleaned))\n        has_chinese = bool(re.search(r'[\\u4e00-\\u9fff]', cleaned))\n        \n        lang_count = sum([has_bangla, has_english, has_chinese])\n        \n        if lang_count > 1:\n            return 'mixed'\n        elif has_chinese:\n            return 'chinese'\n        elif has_bangla:\n            return 'bangla'\n        else:\n            return 'english'\n    except Exception:\n        return 'english'\n\n\ndef detect_all_languages_in_text(text: str) -> List[str]:\n    \"\"\"Detect all languages present in text and return them as a list of language names.\"\"\"\n    if not text:\n        return []\n    \n    # Clean text to remove mathematical notation\n    cleaned = clean_text_for_language_analysis(text, COT_PHRASES)\n    \n    languages_found = []\n    \n    # Check for Bangla (Bengali script)\n    if re.search(r'[\\u0980-\\u09FF]', cleaned):\n        languages_found.append('Bangla')\n    \n    # Check for English (Latin alphabet)\n    if re.search(r'[a-zA-Z]{2,}', cleaned):\n        languages_found.append('English')\n    \n    # Check for Chinese (CJK Unified Ideographs)\n    if re.search(r'[\\u4e00-\\u9fff]', cleaned):\n        languages_found.append('Chinese')\n    \n    # Check for Arabic\n    if re.search(r'[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF]', cleaned):\n        languages_found.append('Arabic')\n    \n    # Check for Japanese Hiragana/Katakana\n    if re.search(r'[\\u3040-\\u309F\\u30A0-\\u30FF]', cleaned):\n        languages_found.append('Japanese')\n    \n    # Check for Korean Hangul\n    if re.search(r'[\\uAC00-\\uD7AF\\u1100-\\u11FF\\u3130-\\u318F]', cleaned):\n        languages_found.append('Korean')\n    \n    # Check for Thai\n    if re.search(r'[\\u0E00-\\u0E7F]', cleaned):\n        languages_found.append('Thai')\n    \n    # Check for Cyrillic (Russian, etc.)\n    if re.search(r'[\\u0400-\\u04FF]', cleaned):\n        languages_found.append('Cyrillic')\n    \n    # Check for Greek\n    if re.search(r'[\\u0370-\\u03FF]', cleaned):\n        languages_found.append('Greek')\n    \n    # If no languages detected, return Unknown\n    if not languages_found:\n        languages_found.append('Unknown')\n    \n    return languages_found\n\n\ndef detect_actual_languages(text: str) -> Dict[str, bool]:\n    \"\"\"Detect actual presence of Bangla, English, and Chinese in text (after cleaning).\"\"\"\n    cleaned = clean_text_for_language_analysis(text, COT_PHRASES)\n    \n    has_bangla = bool(re.search(r'[\\u0980-\\u09FF]', cleaned))\n    has_english = bool(re.search(r'[a-zA-Z]{2,}', cleaned))\n    has_chinese = bool(re.search(r'[\\u4e00-\\u9fff]', cleaned))\n    \n    return {\n        'bangla': has_bangla,\n        'english': has_english,\n        'chinese': has_chinese\n    }\n\n\ndef get_language_combination_label(langs: Dict[str, bool]) -> str:\n    \"\"\"Convert language presence dict to a readable combination string.\"\"\"\n    present = [lang.capitalize() for lang, is_present in sorted(langs.items()) if is_present]\n    if len(present) == 0:\n        return \"Unknown\"\n    elif len(present) == 1:\n        return present[0] + \" Only\"\n    else:\n        return \"+\".join(present)\n\n\ndef analyze_mixed_compositions(classification_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"Analyze the composition of problems classified as 'mixed'.\"\"\"\n    from collections import Counter\n    \n    mixed_problems = [r for r in classification_results if r.get('language_classification', {}).get('language') == 'mixed']\n    \n    if not mixed_problems:\n        return {\n            'total_mixed': 0,\n            'combinations': {},\n            'chinese_in_mixed': 0,\n            'all_languages_found': [],\n            'details': []\n        }\n    \n    combination_counter = Counter()\n    all_languages_set = set()\n    chinese_count = 0\n    mixed_details = []\n    \n    for problem in mixed_problems:\n        generated_answer = problem.get('generated_answer', '')\n        lang_class = problem.get('language_classification', {})\n        \n        # Detect all languages present\n        languages_list = detect_all_languages_in_text(str(generated_answer))\n        all_languages_set.update(languages_list)\n        \n        # Create combination label\n        if languages_list:\n            combination = \"+\".join(sorted(languages_list))\n        else:\n            combination = \"Unknown\"\n        \n        combination_counter[combination] += 1\n        \n        # Check for Chinese specifically\n        if 'Chinese' in languages_list:\n            chinese_count += 1\n        \n        mixed_details.append({\n            'problem_index': problem.get('problem_index', 'unknown'),\n            'languages': languages_list,\n            'combination': combination,\n            'confidence': lang_class.get('confidence'),\n            'text_preview': str(generated_answer)[:200] if generated_answer else ''\n        })\n    \n    return {\n        'total_mixed': len(mixed_problems),\n        'combinations': dict(combination_counter),\n        'all_languages_found': sorted(list(all_languages_set)),\n        'chinese_in_mixed': chinese_count,\n        'chinese_percentage': round((chinese_count / len(mixed_problems) * 100), 2) if mixed_problems else 0,\n        'details': mixed_details\n    }\n\n\ndef analyze_all_language_presence(classification_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"Analyze actual language presence across all classifications.\"\"\"\n    classification_analysis = {\n        'bangla': {'bangla': 0, 'english': 0, 'chinese': 0, 'total': 0},\n        'english': {'bangla': 0, 'english': 0, 'chinese': 0, 'total': 0},\n        'chinese': {'bangla': 0, 'english': 0, 'chinese': 0, 'total': 0},\n        'mixed': {'bangla': 0, 'english': 0, 'chinese': 0, 'total': 0}\n    }\n    \n    for problem in classification_results:\n        lang_class = problem.get('language_classification', {})\n        classified_as = lang_class.get('language', '')\n        generated_answer = problem.get('generated_answer', '')\n        \n        if classified_as in classification_analysis:\n            classification_analysis[classified_as]['total'] += 1\n            langs = detect_actual_languages(str(generated_answer))\n            for lang, present in langs.items():\n                if present:\n                    classification_analysis[classified_as][lang] += 1\n    \n    return classification_analysis\n\n\n# ---------------------------\n# Prompt Template\n# ---------------------------\nLANGUAGE_CLASSIFICATION_PROMPT = Template(r\"\"\"You are an expert language classifier specializing in distinguishing between Bangla, English, and Chinese text in mathematical solutions.\n\nTASK: Decide whether the provided solution text is written in Bangla, English, Chinese, or Mixed.\n\nIMPORTANT RULES (follow exactly):\n- Respond with ONLY one plain JSON object (no Markdown, no code fences, no extra text).\n- The JSON must be FLAT (no arrays/lists and no nested objects/dicts).\n- For phrase fields (bangla_phrases, english_phrases, chinese_phrases, excluded_cot_phrases) return a single string containing phrases separated by semicolons (`;`). If none, return an empty string.\n- Allowed keys (exact): \"language\", \"confidence\", \"bangla_phrases\", \"english_phrases\", \"chinese_phrases\", \"excluded_cot_phrases\", \"mixed_languages_present\". Do NOT include extra keys.\n- \"language\" must be one of: \"bangla\", \"english\", \"chinese\", \"mixed\" (lowercase).\n- \"confidence\" must be a number between 0.0 and 1.0 (use 0.0-1.0). If unknown, return 0.0.\n- \"mixed_languages_present\" should be a semicolon-separated string of languages found ONLY if language is \"mixed\". For single-language classifications, return empty string. Examples: \"Bangla;English\" or \"Chinese;English;Bangla\" or \"\".\n- Do not include explanatory text or commentary.\n\nLANGUAGE DETECTION GUIDELINES:\n- Bangla: Uses Bengali script (Unicode range U+0980-U+09FF)\n- English: Uses Latin alphabet (a-z, A-Z)\n- Chinese: Uses Chinese characters (CJK Unified Ideographs, e.g., 这是中文)\n- Mixed: Contains 2 or more languages - YOU MUST specify which languages in \"mixed_languages_present\"\n\nEXCLUSIONS (ignore while deciding language): mathematical notation, digits, CoT phrases, and <think> content.\n\nORIGINAL SOLUTION TEXT:\n$original_text\n\nCLEANED TEXT FOR ANALYSIS:\n$cleaned_text\n\nExample valid responses:\n\nFor single language:\n{\"language\":\"bangla\",\"confidence\":0.95,\"bangla_phrases\":\"প্রথমে আমরা\",\"english_phrases\":\"\",\"chinese_phrases\":\"\",\"excluded_cot_phrases\":\"\",\"mixed_languages_present\":\"\"}\n\nFor mixed language:\n{\"language\":\"mixed\",\"confidence\":0.88,\"bangla_phrases\":\"আমরা এই সমস্যা\",\"english_phrases\":\"We need to find\",\"chinese_phrases\":\"首先分析\",\"excluded_cot_phrases\":\"step 1\",\"mixed_languages_present\":\"Bangla;English;Chinese\"}\n\nJSON ONLY.\"\"\")\n\n# ---------------------------\n# Classification Logic\n# ---------------------------\n\ndef classify_language_with_gemini(api_manager: GeminiLanguageApiManager, generated_answer: Any, problem_index: int, model_name: str = DEFAULT_MODEL) -> Dict[str, Any]:\n    \"\"\"Classify the language of a generated answer using Gemini API.\"\"\"\n    try:\n        if isinstance(generated_answer, list):\n            generated_answer_text = str(generated_answer[0]) if generated_answer else \"\"\n        else:\n            generated_answer_text = str(generated_answer or \"\")\n\n        if not generated_answer_text:\n            return {\n                'response_text': 'Input validation failed',\n                'language': 'english',\n                'confidence': None,\n                'bangla_phrases': [],\n                'english_phrases': [],\n                'chinese_phrases': [],\n                'excluded_cot_phrases': [],\n                'mixed_languages_present': '',\n            }\n\n        cleaned_text = clean_text_for_language_analysis(generated_answer_text, COT_PHRASES)\n\n        try:\n            prompt = LANGUAGE_CLASSIFICATION_PROMPT.safe_substitute(\n                original_text=generated_answer_text[:4000],\n                cleaned_text=cleaned_text[:2000]\n            )\n        except Exception as e:\n            logging.error(f\"Prompt substitution failed for problem {problem_index}: {e}\")\n            prompt = f\"ORIGINAL:\\n{generated_answer_text[:2000]}\\n\\nCLEANED:\\n{cleaned_text[:1000]}\"\n\n        # send prompt to model\n        response = api_manager.generate_content(model=model_name, contents=prompt)\n\n        # Robustly extract text field from SDK response\n        response_text = ''\n        if hasattr(response, 'text'):\n            response_text = getattr(response, 'text') or ''\n        elif isinstance(response, dict) and 'text' in response:\n            response_text = response.get('text') or ''\n        else:\n            response_text = str(response)\n\n        response_text = response_text.strip()\n\n        language, confidence, bangla_phrases, english_phrases, chinese_phrases, excluded_cot_phrases, mixed_languages_present = extract_language_classification(response_text)\n\n        if language not in ('bangla', 'english', 'chinese', 'mixed'):\n            language = get_heuristic_classification(generated_answer_text, COT_PHRASES)\n            logging.info(f\"Problem {problem_index}: heuristic fallback -> {language}\")\n\n        return {\n            'response_text': response_text,\n            'language': language,\n            'confidence': confidence,\n            'bangla_phrases': bangla_phrases,\n            'english_phrases': english_phrases,\n            'chinese_phrases': chinese_phrases,\n            'excluded_cot_phrases': excluded_cot_phrases,\n            'mixed_languages_present': mixed_languages_present,\n        }\n\n    except Exception as e:\n        logging.error(f\"Error classifying language for problem {problem_index}: {e}\")\n        try:\n            heuristic_lang = get_heuristic_classification(str(generated_answer or ''), COT_PHRASES)\n        except Exception:\n            heuristic_lang = 'english'\n        return {\n            'response_text': f'Error: {e}',\n            'language': heuristic_lang,\n            'confidence': None,\n            'bangla_phrases': [],\n            'english_phrases': [],\n            'chinese_phrases': [],\n            'excluded_cot_phrases': [],\n            'mixed_languages_present': '',\n        }\n\n# ---------------------------\n# I/O Helpers\n# ---------------------------\n\ndef save_json_atomic(obj: Dict, path: str) -> None:\n    \"\"\"Save JSON file atomically using a temporary file.\"\"\"\n    tmp = path + '.tmp'\n    with open(tmp, 'w', encoding='utf-8') as f:\n        json.dump(obj, f, indent=2, ensure_ascii=False)\n    os.replace(tmp, path)\n\n\ndef _load_input_data(path: str) -> List[Dict[str, Any]]:\n    \"\"\"Load input data from JSON or JSONL file.\"\"\"\n    if path.endswith('.jsonl') or path.endswith('.ndjson'):\n        data: List[Dict[str, Any]] = []\n        with open(path, 'r', encoding='utf-8') as f:\n            for line_no, line in enumerate(f, start=1):\n                line = line.strip()\n                if not line:\n                    continue\n                try:\n                    data.append(json.loads(line))\n                except json.JSONDecodeError as je:\n                    logging.warning(f\"Skipping invalid JSON on line {line_no} in {path}: {je}\")\n        return data\n    else:\n        with open(path, 'r', encoding='utf-8') as f:\n            return json.load(f)\n\n\ndef calculate_confidence_stats(classification_results: List[Dict]) -> Dict:\n    \"\"\"Calculate statistics for confidence scores.\"\"\"\n    confidences: List[float] = []\n    for r in classification_results:\n        lc = r.get('language_classification') or {}\n        conf = lc.get('confidence')\n        if conf is not None:\n            confidences.append(conf)\n    if not confidences:\n        return {\"count\": 0}\n    confidences_sorted = sorted(confidences)\n    return {\n        \"count\": len(confidences),\n        \"mean\": sum(confidences) / len(confidences),\n        \"min\": min(confidences),\n        \"max\": max(confidences),\n        \"median\": confidences_sorted[len(confidences_sorted) // 2],\n    }\n\n\n# ---------------------------\n# Main processing function\n# ---------------------------\ndef process_json_file(api_manager: GeminiLanguageApiManager, input_path: str, output_path: str, model_name: str = DEFAULT_MODEL, checkpoint_interval: int = 50) -> None:\n    \"\"\"Process JSON file and classify languages for all problems.\"\"\"\n    json_data = _load_input_data(input_path)\n    logging.info(f\"Loaded {len(json_data)} problems from {input_path}\")\n\n    classification_results: List[Dict[str, Any]] = []\n    total_problems = len(json_data)\n    processed_problems = 0\n\n    bangla_count = english_count = chinese_count = mixed_count = 0\n    problems_with_confidence = 0\n\n    for idx, problem in enumerate(tqdm(json_data, desc='Classifying languages')):\n        try:\n            problem_index = problem.get('problem_index', idx)\n            generated_answer = problem.get('generated_answer', '')\n\n            if not generated_answer:\n                logging.warning(f\"Skipping problem {problem_index}: missing generated answer\")\n                entry = problem.copy()\n                entry['language_classification'] = {\n                    'language': None,\n                    'confidence': None,\n                    'bangla_phrases': [],\n                    'english_phrases': [],\n                    'chinese_phrases': [],\n                    'excluded_cot_phrases': [],\n                    'classification_reason': 'Missing generated answer',\n                    'response_text': 'Missing generated answer',\n                }\n                classification_results.append(entry)\n                continue\n\n            result = classify_language_with_gemini(api_manager, generated_answer, problem_index, model_name)\n\n            language = result['language']\n            confidence = result.get('confidence')\n\n            entry = problem.copy()\n            entry['language_classification'] = {\n                'language': language,\n                'confidence': confidence,\n                'bangla_phrases': result.get('bangla_phrases', []),\n                'english_phrases': result.get('english_phrases', []),\n                'chinese_phrases': result.get('chinese_phrases', []),\n                'excluded_cot_phrases': result.get('excluded_cot_phrases', []),\n                'mixed_languages_present': result.get('mixed_languages_present', ''),\n                'response_text': result.get('response_text', ''),\n            }\n            classification_results.append(entry)\n\n            processed_problems += 1\n            if language == 'bangla':\n                bangla_count += 1\n            elif language == 'english':\n                english_count += 1\n            elif language == 'chinese':\n                chinese_count += 1\n            elif language == 'mixed':\n                mixed_count += 1\n\n            if confidence is not None:\n                problems_with_confidence += 1\n\n            if confidence is not None:\n                print(f\"Problem {problem_index}: {language.upper()} (conf: {confidence:.2f})\")\n            else:\n                print(f\"Problem {problem_index}: {language.upper()}\")\n\n            if processed_problems % checkpoint_interval == 0:\n                checkpoint_file = output_path.replace('.json', f'_checkpoint_{processed_problems}.json')\n                save_json_atomic(classification_results, checkpoint_file)\n                stats = api_manager.get_usage_stats()\n                logging.info(f\"Progress: {processed_problems}/{total_problems}\")\n                logging.info(f\"Distribution - Bangla: {bangla_count}, English: {english_count}, Chinese: {chinese_count}, Mixed: {mixed_count}\")\n                logging.info(f\"API Usage: {stats['total_used']}/{stats['total_available']} ({stats['percent_used']:.1f}%)\")\n\n        except Exception as e:\n            logging.error(f\"Error processing problem {problem.get('problem_index', idx)}: {e}\")\n            heuristic_lang = get_heuristic_classification(str(problem.get('generated_answer', '')), COT_PHRASES)\n            entry = problem.copy()\n            entry['language_classification'] = {\n                'language': heuristic_lang,\n                'confidence': None,\n                'bangla_phrases': [],\n                'english_phrases': [],\n                'chinese_phrases': [],\n                'excluded_cot_phrases': [],\n                'classification_reason': f'Error: {e}',\n                'response_text': f'Error: {e}',\n            }\n            classification_results.append(entry)\n            continue\n\n    confidence_rate = (problems_with_confidence / processed_problems) * 100 if processed_problems else 0\n    confidence_stats = calculate_confidence_stats(classification_results)\n    \n    # Analyze mixed language compositions\n    logging.info(\"Analyzing mixed language compositions...\")\n    mixed_analysis = analyze_mixed_compositions(classification_results)\n    \n    # Analyze language presence across all classifications\n    logging.info(\"Analyzing language presence across all classifications...\")\n    language_presence_analysis = analyze_all_language_presence(classification_results)\n\n    final_results = {\n        'classification_metadata': {\n            'model_used': model_name,\n            'total_problems': total_problems,\n            'successfully_processed': processed_problems,\n            'language_distribution': {\n                'bangla': bangla_count, \n                'english': english_count, \n                'chinese': chinese_count,\n                'mixed': mixed_count\n            },\n            'language_percentages': {\n                'bangla': round((bangla_count / processed_problems) * 100, 2) if processed_problems else 0,\n                'english': round((english_count / processed_problems) * 100, 2) if processed_problems else 0,\n                'chinese': round((chinese_count / processed_problems) * 100, 2) if processed_problems else 0,\n                'mixed': round((mixed_count / processed_problems) * 100, 2) if processed_problems else 0,\n            },\n            'problems_with_confidence': problems_with_confidence,\n            'confidence_rate_percentage': round(confidence_rate, 2),\n            'confidence_statistics': confidence_stats,\n            'mixed_language_analysis': {\n                'total_mixed_problems': mixed_analysis['total_mixed'],\n                'all_languages_found': mixed_analysis.get('all_languages_found', []),\n                'language_combinations': mixed_analysis['combinations'],\n                'chinese_in_mixed_count': mixed_analysis['chinese_in_mixed'],\n                'chinese_in_mixed_percentage': mixed_analysis['chinese_percentage'],\n            },\n            'language_presence_by_classification': language_presence_analysis,\n            'excluded_cot_phrases': COT_PHRASES,\n            'classification_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n        },\n        'problem_classifications': classification_results,\n        'mixed_problem_details': mixed_analysis['details']\n    }\n\n    save_json_atomic(final_results, output_path)\n\n    # Summary print\n    print('\\n' + '='*60)\n    print('LANGUAGE CLASSIFICATION COMPLETED!')\n    print('='*60)\n    print(f\"Model Used: {model_name}\")\n    print(f\"Total Problems: {total_problems}\")\n    print(f\"Successfully Processed: {processed_problems}\")\n    if processed_problems:\n        print(f\"  - Bangla: {bangla_count} ({(bangla_count/processed_problems)*100:.1f}%)\")\n        print(f\"  - English: {english_count} ({(english_count/processed_problems)*100:.1f}%)\")\n        print(f\"  - Chinese: {chinese_count} ({(chinese_count/processed_problems)*100:.1f}%)\")\n        print(f\"  - Mixed: {mixed_count} ({(mixed_count/processed_problems)*100:.1f}%)\")\n    print(f\"Problems with Confidence: {problems_with_confidence} ({confidence_rate:.1f}%)\")\n    if confidence_stats.get('count', 0) > 0:\n        print(f\"Average Confidence: {confidence_stats['mean']:.3f}\")\n        print(f\"Confidence Range: {confidence_stats['min']:.3f} - {confidence_stats['max']:.3f}\")\n    \n    # Print mixed language analysis\n    if mixed_count > 0:\n        print('\\n' + '='*60)\n        print('MIXED LANGUAGE COMPOSITION ANALYSIS')\n        print('='*60)\n        print(f\"Total Mixed Problems: {mixed_analysis['total_mixed']}\")\n        \n        # Show all unique languages found\n        all_langs = mixed_analysis.get('all_languages_found', [])\n        if all_langs:\n            print(f\"\\nAll Languages Found in Mixed: {', '.join(all_langs)}\")\n        \n        print(f\"\\nChinese in Mixed: {mixed_analysis['chinese_in_mixed']} ({mixed_analysis['chinese_percentage']:.1f}%)\")\n        print(\"\\nLanguage Combinations:\")\n        for combination, count in sorted(mixed_analysis['combinations'].items(), key=lambda x: x[1], reverse=True):\n            percentage = (count / mixed_analysis['total_mixed'] * 100) if mixed_analysis['total_mixed'] > 0 else 0\n            print(f\"  {combination:40s}: {count:4d} ({percentage:5.1f}%)\")\n        \n        # Show sample problems for each combination\n        print(\"\\n\" + \"-\"*60)\n        print(\"Sample Problems by Combination:\")\n        print(\"-\"*60)\n        \n        # Group details by combination\n        combo_examples = defaultdict(list)\n        for detail in mixed_analysis['details']:\n            combo_examples[detail['combination']].append(detail)\n        \n        # Show top 3 combinations with examples\n        top_combinations = sorted(mixed_analysis['combinations'].items(), key=lambda x: x[1], reverse=True)[:3]\n        for combination, count in top_combinations:\n            print(f\"\\n{combination} ({count} problems):\")\n            examples = combo_examples[combination][:2]  # Show 2 examples\n            for i, ex in enumerate(examples, 1):\n                print(f\"  Example {i} - Problem {ex['problem_index']}:\")\n                print(f\"    Languages: {', '.join(ex['languages'])}\")\n                if ex.get('confidence'):\n                    print(f\"    Confidence: {ex['confidence']:.2f}\")\n                preview = ex.get('text_preview', '')[:100].replace('\\n', ' ')\n                if preview:\n                    print(f\"    Preview: {preview}...\")\n    \n    # Print language presence analysis\n    print('\\n' + '='*60)\n    print('LANGUAGE PRESENCE BY CLASSIFICATION')\n    print('='*60)\n    print(f\"{'Classified As':<15} | {'Total':>6} | {'Bangla':>7} | {'English':>7} | {'Chinese':>7}\")\n    print('-'*60)\n    for classification in ['bangla', 'english', 'chinese', 'mixed']:\n        analysis = language_presence_analysis.get(classification, {})\n        total = analysis.get('total', 0)\n        if total > 0:\n            print(f\"{classification.capitalize():<15} | {total:>6d} | {analysis.get('bangla', 0):>7d} | {analysis.get('english', 0):>7d} | {analysis.get('chinese', 0):>7d}\")\n    \n    print(f\"\\nCoT Phrases Excluded: {len(COT_PHRASES)} phrases\")\n    print(f\"Output saved to: {output_path}\")\n\n    final_stats = api_manager.get_usage_stats()\n    print(f\"Final API Usage: {final_stats['total_used']}/{final_stats['total_available']} ({final_stats['percent_used']:.1f}%)\")\n    print('='*60)\n\n# ---------------------------\n# Small utilities / validators\n# ---------------------------\n\ndef validate_json_structure(file_path: str) -> bool:\n    \"\"\"Validate the structure of the input JSON file.\"\"\"\n    try:\n        data = _load_input_data(file_path)\n        if not isinstance(data, list):\n            print(f\"Warning: {file_path} should contain a list of problems\")\n            return False\n        if not data:\n            print(f\"Warning: {file_path} is empty\")\n            return False\n        for i in range(min(3, len(data))):\n            problem = data[i]\n            if not isinstance(problem, dict):\n                print(f\"Warning: Problem {i} is not a dictionary\")\n                return False\n            if 'generated_answer' not in problem:\n                print(f\"Warning: Problem {i}: missing 'generated_answer'\")\n                return False\n        print(f\"✅ {file_path} structure validation passed\")\n        return True\n    except Exception as e:\n        print(f\"Error validating {file_path}: {e}\")\n        return False\n\n# ---------------------------\n# Main Execution\n# ---------------------------\n\ndef main():\n    \"\"\"Main execution function.\"\"\"\n    gemini_api_path = \"/kaggle/input/part_6_api_key.txt\"  # update if needed (single path)\n    input_path = \"/kaggle/input/Code Switching Files/Mathstral_7B_Q(E)_CoT(B).json\"\n    output_path = \"/kaggle/working/Mathstral_7B_Q(E)_CoT(B)_Language_Classification.json\"\n    model_name = DEFAULT_MODEL\n    calls_per_day = DEFAULT_CALLS_PER_DAY\n    rate_limit_delay = DEFAULT_RATE_LIMIT\n    checkpoint_interval = 50\n\n    print(\"=\"*60)\n    print(\"GEMINI LANGUAGE CLASSIFICATION TOOL\")\n    print(\"WITH CHINESE DETECTION\")\n    print(\"=\"*60)\n\n    api_keys = load_api_keys(key_path=gemini_api_path)\n    api_manager = GeminiLanguageApiManager(api_keys, calls_per_day=calls_per_day, rate_limit_delay=rate_limit_delay)\n\n    validate_json_structure(input_path)\n\n    process_json_file(api_manager, input_path, output_path, model_name=model_name, checkpoint_interval=checkpoint_interval)\n\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}