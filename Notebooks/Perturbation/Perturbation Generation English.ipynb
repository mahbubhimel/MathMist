{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13166302,"sourceType":"datasetVersion","datasetId":8147915},{"sourceId":13167854,"sourceType":"datasetVersion","datasetId":8244681}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:49:22.386969Z","iopub.execute_input":"2025-09-25T09:49:22.387686Z","iopub.status.idle":"2025-09-25T09:49:22.721908Z","shell.execute_reply.started":"2025-09-25T09:49:22.387656Z","shell.execute_reply":"2025-09-25T09:49:22.721012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Suppose you saved keys in /content/drive/MyDrive/gemini_keys.txt as single line: key1,key2,...\nwith open('/kaggle/input/numerical-symbolic-nctb/gemini_keys.txt','r') as fh:\n    keys = fh.read().strip()\n\nimport os\nimport re\nimport time\nimport json\nimport queue\nimport random\nimport logging\nimport threading\nimport traceback\nfrom tqdm import tqdm\nfrom collections import deque\nfrom datetime import datetime, timezone\nfrom google import genai\nfrom typing import Optional, Dict, Any\n\n# ---------------------------\n# Logging\n# ---------------------------\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(\"english_solution_perturber.log\"),\n        logging.StreamHandler()\n    ]\n)\n\n# If keys were loaded above, set them into an environment variable for optional use by other code.\nif 'keys' in globals() and keys:\n    os.environ['GEMINI_API_KEYS'] = keys\n    print(\"Loaded keys from Drive into environment variable GEMINI_API_KEYS.\")\nelse:\n    print(\"No keys loaded from Drive; will attempt to read GEMINI_API_KEYS environment variable.\")\n\n# ---------------------------\n# Gemini API manager\n# ---------------------------\nclass GeminiSolutionApiManager:\n    \"\"\"\n    Manages multiple Gemini API keys with rotation and rate limiting for solution perturbation tasks.\n    Thread-safe: uses a lock to guard key/usage state.\n    \"\"\"\n\n    def __init__(self, api_keys, calls_per_day=1000, rate_limit_delay=4):\n        if not api_keys:\n            raise ValueError(\"api_keys must contain at least one key\")\n\n        self.api_keys = deque(api_keys)\n        self.calls_per_day = calls_per_day\n        self.rate_limit_delay = rate_limit_delay\n\n        # Track usage for each key\n        self.usage_count = {key: 0 for key in api_keys}\n        self.current_key = self.api_keys[0]\n        self.client = genai.Client(api_key=self.current_key)\n\n        # Thread-safety\n        self.lock = threading.Lock()\n\n        # Set up a queue for API calls\n        self.call_queue = queue.Queue()\n        self.worker_thread = threading.Thread(target=self._process_queue, name=\"GeminiWorker\")\n        self.worker_thread.daemon = True\n        self.worker_thread.start()\n\n        logging.info(f\"Solution API Manager initialized with {len(api_keys)} keys\")\n\n    def _rotate_key(self):\n        \"\"\"Rotate to the next available API key (thread-safe).\"\"\"\n        with self.lock:\n            self.api_keys.rotate(1)\n            self.current_key = self.api_keys[0]\n            self.client = genai.Client(api_key=self.current_key)\n            usage = self.usage_count.get(self.current_key, 0)\n        logging.info(f\"Rotated to new API key (usage: {usage})\")\n\n    def _find_available_key(self):\n        \"\"\"Find an API key that hasn't reached the daily limit. Returns True if found.\"\"\"\n        with self.lock:\n            if self.usage_count.get(self.current_key, 0) < self.calls_per_day:\n                return True\n\n        initial_key = self.current_key\n        for _ in range(len(self.api_keys)):\n            self._rotate_key()\n            with self.lock:\n                if self.usage_count.get(self.current_key, 0) < self.calls_per_day:\n                    return True\n            if self.current_key == initial_key:\n                return False\n        return False\n\n    def _process_queue(self):\n        \"\"\"Process the queue of API calls in a background worker.\"\"\"\n        while True:\n            try:\n                args, kwargs, result_queue = self.call_queue.get()\n\n                # Find available key\n                if not self._find_available_key():\n                    err = {\"error\": \"All API keys have reached their daily limit\"}\n                    try:\n                        result_queue.put(err)\n                    except Exception:\n                        logging.exception(\"Failed to notify caller about exhausted keys\")\n                    self.call_queue.task_done()\n                    time.sleep(10)\n                    continue\n\n                try:\n                    response = None\n                    try:\n                        # call into Gemini client (args and kwargs forwarded)\n                        response = self.client.models.generate_content(*args, **kwargs)\n                    except Exception as api_exc:\n                        msg = str(api_exc).lower()\n                        if 'quota' in msg or 'rate limit' in msg or 'quota_exceeded' in msg:\n                            with self.lock:\n                                self.usage_count[self.current_key] = self.calls_per_day\n                            logging.warning(f\"API key reached quota/rate-limit: {api_exc}\")\n                            result_queue.put({\"error\": f\"Rate limit/quota: {api_exc}\"})\n                        else:\n                            logging.error(f\"API call error: {api_exc}\")\n                            result_queue.put({\"error\": str(api_exc)})\n                        response = None\n\n                    if response is not None:\n                        result_queue.put({\"response\": response})\n                        with self.lock:\n                            self.usage_count[self.current_key] = self.usage_count.get(self.current_key, 0) + 1\n\n                except Exception as e:\n                    logging.error(f\"Unexpected API invocation error: {e}\\n{traceback.format_exc()}\")\n                    try:\n                        result_queue.put({\"error\": str(e)})\n                    except Exception:\n                        logging.exception(\"Failed to send error to result queue\")\n\n                time.sleep(self.rate_limit_delay)\n                self.call_queue.task_done()\n\n            except Exception as e:\n                logging.error(f\"Queue processing error: {e}\\n{traceback.format_exc()}\")\n                time.sleep(1)\n                continue\n\n    def generate_content(self, *args, timeout=60, **kwargs):\n        \"\"\"Make an API call to generate content, automatically handling key rotation.\"\"\"\n        result_queue = queue.Queue()\n        self.call_queue.put((args, kwargs, result_queue))\n        try:\n            result = result_queue.get(timeout=timeout)\n        except queue.Empty:\n            raise TimeoutError(\"Timed out waiting for API worker result.\")\n\n        if \"error\" in result:\n            raise Exception(result[\"error\"])\n        return result[\"response\"]\n\n    def reset_usage_counts(self):\n        \"\"\"Reset the usage counts for all keys (e.g., at the start of a new day).\"\"\"\n        with self.lock:\n            self.usage_count = {key: 0 for key in self.api_keys}\n        logging.info(\"Reset API key usage counts\")\n\n    def get_usage_stats(self):\n        \"\"\"Get usage statistics for all keys.\"\"\"\n        with self.lock:\n            per_key = dict(self.usage_count)\n        total_used = sum(per_key.values())\n        total_available = len(self.api_keys) * self.calls_per_day\n        return {\n            \"per_key\": per_key,\n            \"total_used\": total_used,\n            \"total_available\": total_available,\n            \"percent_used\": (total_used / total_available) * 100 if total_available > 0 else 0\n        }\n\n# ---------------------------\n# Robust Gemini JSON extraction\n# ---------------------------\n\ndef parse_gemini_json_object(text: str) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Robustly extract the first JSON object from Gemini output.\n    1) Look for fenced code blocks (```json ... ``` and variants) and try each match.\n    2) For each candidate try json.loads; if it fails, escape stray backslashes and retry.\n    3) If no fenced block parses, fall back to a balanced-brace scan of the entire text.\n    Returns a dict on success, or None on failure.\n    \"\"\"\n    if not text:\n        return None\n\n    # Normalize whitespace a little\n    text = text.strip()\n\n    # Combined code-fence regex that covers:\n    #  ```json\\n ... \\n```\n    #  ```json ... ```\n    #  ```\\n ... \\n```\n    #  ``` ... ```\n    fence_re = re.compile(r'```(?:json)?\\s*\\n?(.*?)\\n?```', re.DOTALL | re.IGNORECASE)\n\n    # Try fenced blocks first (if any). re.findall returns all non-overlapping matches.\n    for match in fence_re.findall(text):\n        candidate = match.strip()\n        if not candidate:\n            continue\n        # Try parsing candidate directly\n        try:\n            parsed = json.loads(candidate)\n            if isinstance(parsed, dict):\n                return parsed\n        except json.JSONDecodeError:\n            # Escape stray backslashes that are NOT valid JSON escapes:\n            # valid escapes: \" \\ / b f n r t u\n            fixed = re.sub(r'\\\\(?![\"\\\\/bfnrtu])', r'\\\\\\\\', candidate)\n            try:\n                parsed = json.loads(fixed)\n                if isinstance(parsed, dict):\n                    return parsed\n            except json.JSONDecodeError:\n                # try next fenced block (if any)\n                continue\n\n    # If no fenced block worked, try direct full-text parse (maybe the whole text is JSON)\n    try:\n        parsed_full = json.loads(text)\n        if isinstance(parsed_full, dict):\n            return parsed_full\n    except Exception:\n        pass\n\n    # Fallback: scan for the first balanced {...} substring (robust to extra text around JSON)\n    start = None\n    depth = 0\n    for i, ch in enumerate(text):\n        if ch == '{':\n            if start is None:\n                start = i\n            depth += 1\n        elif ch == '}' and start is not None:\n            depth -= 1\n            if depth == 0:\n                candidate = text[start:i+1]\n                try:\n                    parsed = json.loads(candidate)\n                    if isinstance(parsed, dict):\n                        return parsed\n                except json.JSONDecodeError:\n                    fixed = re.sub(r'\\\\(?![\"\\\\/bfnrtu])', r'\\\\\\\\', candidate)\n                    try:\n                        parsed = json.loads(fixed)\n                        if isinstance(parsed, dict):\n                            return parsed\n                    except json.JSONDecodeError:\n                        # if this candidate fails, continue scanning (rare)\n                        start = None\n                        depth = 0\n                        continue\n    return None\n\n# ---------------------------\n# Enhanced Solution Perturbation Prompt Templates (3 Strategies)\n# ---------------------------\n\nSTRATEGY_PROMPTS = {\n    1: \"\"\"You are a mathematics expert creating a sophisticated flawed solution by combining multiple error types.\n\nTASK: Create a flawed solution combining THREE specific error types while maintaining the correct final answer:\n1) STEP OMISSION: Skip one crucial step naturally\n2) INCORRECT RULE/THEOREM: Apply one wrong mathematical rule confidently\n3) FAULTY CAUSAL REASONING: Make incorrect cause-effect assumptions\n\nQuestion: {question}\nCorrect Solution: {solution}\nFinal Correct Answer: {exact_answer}\n\n🎯 CRITICAL INSTRUCTIONS:\n- Write the solution in the SAME STYLE and linguistic pattern as the Correct Solution\n- Seamlessly integrate all three error types without any meta-commentary\n- Present everything confidently as if completely correct\n- DO NOT mention or indicate that anything is wrong\n- Final answer MUST exactly match: {exact_answer}\n- Mimic the exact tone, structure, and presentation style of the Correct Solution\n\n⚠️ The last line of the solution MUST be exactly: \"{exact_answer}\" — do not add, remove, or change any character (no extra spaces or newlines).\n⚠️ THE SOLUTION SHOULD LOOK COMPLETELY AUTHORITATIVE - multiple errors should be naturally embedded.\n\nEXAMPLES OF ERROR COMBINATIONS TO EMBED NATURALLY:\n- Skip a verification step + use wrong formula + assume incorrect causation\n- Omit algebraic step + apply wrong theorem + make faulty inference\n- Skip condition check + wrong property + incorrect reasoning\n\nREQUIRED JSON FORMAT:\n{{\"modified_solution\": \"Natural-sounding solution with three error types (no meta-commentary)\", \"error_description\": \"Brief 2-line summary of the three error types combined\", \"erroneous_part\": \"Locations where the three errors occur\"}}\"\"\",\n\n    2: \"\"\"You are a mathematics expert creating a sophisticated flawed solution by combining two error types.\n\nTASK: Create a flawed solution combining TWO specific error types while maintaining the correct final answer:\n1) OVERGENERALIZATION: Make sweeping generalizations from limited cases\n2) LOGICAL FALLACIES: Use incorrect logical reasoning patterns\n\nQuestion: {question}\nCorrect Solution: {solution}\nFinal Correct Answer: {exact_answer}\n\n🎯 CRITICAL INSTRUCTIONS:\n- Write the solution in the SAME STYLE and linguistic pattern as the Correct Solution\n- Seamlessly integrate both error types without any meta-commentary\n- Present everything confidently as if completely correct\n- DO NOT mention or indicate that anything is wrong\n- Final answer MUST exactly match: {exact_answer}\n- Mimic the exact tone, structure, and presentation style of the Correct Solution\n\n⚠️ The last line of the solution MUST be exactly: \"{exact_answer}\" — do not add, remove, or change any character (no extra spaces or newlines).\n⚠️ THE SOLUTION SHOULD LOOK COMPLETELY AUTHORITATIVE - both errors should be naturally embedded.\n\nEXAMPLES OF ERROR COMBINATIONS TO EMBED NATURALLY:\n- Generalize from one case + use invalid if-then logic\n- Assume pattern holds everywhere + make incorrect logical connections\n- Overgeneralize from examples + use faulty deductive reasoning\n\nREQUIRED JSON FORMAT:\n{{\"modified_solution\": \"Natural-sounding solution with two error types (no meta-commentary)\", \"error_description\": \"Brief 2-line summary of the two error types combined\", \"erroneous_part\": \"Locations where the two errors occur\"}}\"\"\",\n\n    3: \"\"\"You are a mathematics expert creating the most sophisticated flawed solution by combining all major error types.\n\nTASK: Create a flawed solution combining ALL FIVE error types while maintaining the correct final answer:\n1) STEP OMISSION: Skip crucial steps naturally\n2) INCORRECT RULE/THEOREM: Apply wrong mathematical rules confidently\n3) FAULTY CAUSAL REASONING: Make incorrect cause-effect assumptions\n4) OVERGENERALIZATION: Make sweeping generalizations from limited cases\n5) LOGICAL FALLACIES: Use incorrect logical reasoning patterns\n\nQuestion: {question}\nCorrect Solution: {solution}\nFinal Correct Answer: {exact_answer}\n\n🎯 CRITICAL INSTRUCTIONS:\n- Write the solution in the SAME STYLE and linguistic pattern as the Correct Solution\n- Seamlessly integrate all five error types without any meta-commentary\n- Present everything confidently as if completely correct\n- DO NOT mention or indicate that anything is wrong\n- Final answer MUST exactly match: {exact_answer}\n- Mimic the exact tone, structure, and presentation style of the Correct Solution\n\n⚠️ The last line of the solution MUST be exactly: \"{exact_answer}\" — do not add, remove, or change any character (no extra spaces or newlines).\n⚠️ THE SOLUTION SHOULD LOOK COMPLETELY AUTHORITATIVE - all errors should be naturally embedded.\n\nEXAMPLES OF COMPREHENSIVE ERROR INTEGRATION:\n- Skip verification + wrong formula + faulty inference + overgeneralize + invalid logic\n- Omit steps + misapply theorem + incorrect causation + assume patterns + logical fallacies\n\nREQUIRED JSON FORMAT:\n{{\"modified_solution\": \"Natural-sounding solution with all five error types (no meta-commentary)\", \"error_description\": \"Brief 2-line summary of the five error types combined\", \"erroneous_part\": \"Locations where the five errors occur\"}}\"\"\"\n}\n\n# ---------------------------\n# Enhanced Validation Functions\n# ---------------------------\n\ndef validate_perturbed_solution(result, original_solution, exact_answer):\n    \"\"\"\n    Enhanced validation to ensure perturbation quality.\n    \"\"\"\n    if not result.get('success', False):\n        return False, \"Generation failed\"\n\n    modified_solution = result.get('modified_solution', '')\n    error_description = result.get('error_description', '')\n    erroneous_part = result.get('erroneous_part', '')\n\n    # Basic content checks\n    if not modified_solution or len(modified_solution.strip()) < 50:\n        return False, \"Modified solution too short or empty\"\n\n    if not error_description or len(error_description.strip()) < 10:\n        return False, \"Error description too brief\"\n\n    # Check error_description is exactly 2 lines\n    # error_lines = error_description.strip().split('\\n')\n    # if len(error_lines) != 2:\n    #     return False, f\"Error description must be exactly 2 lines, got {len(error_lines)} lines\"\n\n    # # Check each line has meaningful content\n    # for i, line in enumerate(error_lines, 1):\n    #     if len(line.strip()) < 15:\n    #         return False, f\"Error description line {i} too brief (minimum 15 characters)\"\n\n    if not erroneous_part or len(erroneous_part.strip()) < 5:\n        return False, \"Erroneous part description too brief\"\n\n    return True, \"Validation passed\"\n\ndef get_strategy_quality_metrics(strategy_num, result):\n    \"\"\"\n    Get quality metrics specific to each strategy type.\n    \"\"\"\n    if not result.get('success', False):\n        return {'quality_score': 0, 'issues': ['Generation failed']}\n\n    modified_solution = result.get('modified_solution', '')\n    error_description = result.get('error_description', '')\n\n    issues = []\n    quality_score = 100  # Start with perfect score, deduct for issues\n\n    # Strategy-specific checks\n    if strategy_num == 1:  # Step omission + Incorrect rule + Faulty reasoning\n        required_keywords = ['omit', 'skip', 'rule', 'theorem', 'formula', 'reasoning', 'causal', 'inference']\n        found_keywords = sum(1 for keyword in required_keywords if keyword.lower() in error_description.lower())\n        if found_keywords < 3:\n            issues.append('Error description does not clearly indicate all three error types')\n            quality_score -= 25\n\n    elif strategy_num == 2:  # Overgeneralization + Logical fallacies\n        required_keywords = ['generaliz', 'pattern', 'logic', 'fallacy', 'reasoning', 'assumption']\n        found_keywords = sum(1 for keyword in required_keywords if keyword.lower() in error_description.lower())\n        if found_keywords < 2:\n            issues.append('Error description does not clearly indicate both error types')\n            quality_score -= 20\n\n    elif strategy_num == 3:  # All five error types\n        required_keywords = ['omit', 'skip', 'rule', 'theorem', 'reasoning', 'generaliz', 'logic', 'fallacy']\n        found_keywords = sum(1 for keyword in required_keywords if keyword.lower() in error_description.lower())\n        if found_keywords < 4:\n            issues.append('Error description does not clearly indicate multiple error types')\n            quality_score -= 30\n\n    # General quality checks\n    if len(modified_solution) < 100:\n        issues.append('Solution too brief')\n        quality_score -= 15\n\n    if modified_solution.count('=') < 2:  # Should have mathematical content\n        issues.append('Insufficient mathematical content')\n        quality_score -= 10\n\n    # Check for exactly 2 lines in error description\n    error_lines = error_description.strip().split('\\n')\n    if len(error_lines) != 2:\n        issues.append(f'Error description must be exactly 2 lines, got {len(error_lines)}')\n        quality_score -= 20\n\n    return {\n        'quality_score': max(0, quality_score),\n        'issues': issues\n    }\n\n# ---------------------------\n# Enhanced Solution perturbation function\n# ---------------------------\n\ndef generate_perturbed_solution(api_manager, question, solution, exact_answer, strategy_num, max_retries=3):\n    \"\"\"\n    Enhanced generation function with better validation and retry logic.\n    \"\"\"\n    if strategy_num not in STRATEGY_PROMPTS:\n        return {\n            'success': False,\n            'error': f'Invalid strategy number: {strategy_num}',\n            'modified_solution': f\"[Error: Invalid strategy {strategy_num}]\",\n            'error_description': \"Invalid strategy number\\nStrategy must be 1, 2, or 3\",\n            'erroneous_part': \"N/A\",\n            'quality_metrics': {'quality_score': 0, 'issues': ['Invalid strategy number']}\n        }\n\n    prompt = STRATEGY_PROMPTS[strategy_num].format(\n        question=question,\n        solution=solution,\n        exact_answer=exact_answer\n    )\n\n    for attempt in range(max_retries):\n        try:\n            response = api_manager.generate_content(\n                model=\"gemini-2.5-flash-lite\",\n                # model=\"gemini-2.5-flash\",\n                contents=prompt,\n                timeout=120  # Increased timeout for complex tasks\n            )\n\n            # Get response text\n            response_text = \"\"\n            if hasattr(response, 'text'):\n                response_text = response.text\n            elif isinstance(response, dict) and 'text' in response:\n                response_text = response['text']\n            else:\n                response_text = str(response)\n\n            response_text = response_text.strip()\n            logging.info(f\"Strategy {strategy_num}, Attempt {attempt + 1}: Got response\")\n\n            # Parse JSON using robust parser\n            parsed = parse_gemini_json_object(response_text)\n\n            if parsed and isinstance(parsed, dict):\n                required_keys = ['modified_solution', 'error_description', 'erroneous_part']\n                if all(key in parsed for key in required_keys):\n                    # Create preliminary result\n                    result = {\n                        'success': True,\n                        'modified_solution': parsed['modified_solution'],\n                        'error_description': parsed['error_description'],\n                        'erroneous_part': parsed['erroneous_part'],\n                        'raw_response': response_text,\n                        'attempt': attempt + 1\n                    }\n\n                    # Validate the result\n                    is_valid, validation_msg = validate_perturbed_solution(result, solution, exact_answer)\n\n                    if is_valid:\n                        # Add quality metrics\n                        quality_metrics = get_strategy_quality_metrics(strategy_num, result)\n                        result['quality_metrics'] = quality_metrics\n\n                        logging.info(f\"Strategy {strategy_num}, Attempt {attempt + 1}: Validation passed (Quality: {quality_metrics['quality_score']})\")\n                        return result\n                    else:\n                        logging.warning(f\"Strategy {strategy_num}, Attempt {attempt + 1}: Validation failed - {validation_msg}\")\n                        if attempt < max_retries - 1:\n                            continue  # Try again\n                else:\n                    logging.warning(f\"Strategy {strategy_num}, Attempt {attempt + 1}: Missing required keys in JSON\")\n\n            if attempt < max_retries - 1:\n                logging.info(f\"Strategy {strategy_num}, Attempt {attempt + 1} failed, retrying...\")\n                time.sleep(2 + attempt)  # Increasing delay with attempts\n\n        except Exception as e:\n            logging.error(f\"Strategy {strategy_num}, Attempt {attempt + 1} error: {e}\")\n            if attempt < max_retries - 1:\n                time.sleep(5 + attempt * 2)  # Longer pause on error, increasing with attempts\n            continue\n\n    # If all attempts failed, return failure result\n    return {\n        'success': False,\n        'error': f'Failed after {max_retries} attempts',\n        'modified_solution': f\"[Error: Strategy {strategy_num} application failed after {max_retries} attempts]\",\n        'error_description': f\"API call or validation failed after {max_retries} attempts\\nUnable to generate valid perturbed solution\",\n        'erroneous_part': \"N/A\",\n        'quality_metrics': {'quality_score': 0, 'issues': [f'Failed after {max_retries} attempts']}\n    }\n\n# ---------------------------\n# JSONL Processing\n# ---------------------------\n\ndef get_field_case_insensitive(obj, *keys):\n    \"\"\"Get field value with case-insensitive key matching.\"\"\"\n    for key in keys:\n        if key in obj:\n            return obj[key]\n        for present_key in obj.keys():\n            if present_key.lower() == key.lower():\n                return obj[present_key]\n    return ''\n\ndef process_jsonl_file(api_manager, input_file_path, output_file_path):\n    \"\"\"Process JSONL file to add perturbed solutions with enhanced validation.\"\"\"\n    logging.info(f\"Starting enhanced solution perturbation for: {input_file_path}\")\n\n    # Count total objects\n    total_objects = 0\n    with open(input_file_path, 'r', encoding='utf-8') as f:\n        for _ in f:\n            total_objects += 1\n\n    logging.info(f\"Found {total_objects} objects to process\")\n\n    # Processing counters\n    processed_count = 0\n    successful_perturbations = 0\n    quality_scores = []\n\n    with open(input_file_path, 'r', encoding='utf-8') as input_file, \\\n         open(output_file_path, 'w', encoding='utf-8') as output_file:\n\n        for line_num, line in enumerate(tqdm(input_file, total=total_objects, desc=\"Generating Enhanced Perturbed Solutions\"), 1):\n            try:\n                obj = json.loads(line.strip())\n\n                # Extract required fields\n                question = get_field_case_insensitive(obj, 'Question', 'question', 'QuestionText')\n                solution = get_field_case_insensitive(obj, 'Solution', 'solution')\n                exact_answer = get_field_case_insensitive(obj, 'Exact Answer', 'ExactAnswer', 'exact answer', 'Answer')\n\n                if not all([question, solution, exact_answer]):\n                    logging.warning(f\"Line {line_num}: Missing required fields\")\n                    # Add placeholder data for missing fields\n                    for i in range(1, 4):  # Changed from range(1, 7) to range(1, 4)\n                        obj[f'Wrong_Solution_Strategy_{i}'] = f\"[Error: Required fields missing]\"\n                        obj[f'Perturbation_Metadata_Strategy_{i}'] = {\n                            'error_type': 'missing_fields',\n                            'error_description': 'Required fields missing\\nCannot generate perturbed solution',\n                            'erroneous_part': 'N/A',\n                            'success': False,\n                            'quality_metrics': {'quality_score': 0, 'issues': ['Required fields missing']}\n                        }\n                else:\n                    # Generate perturbed solutions for 3 strategies (changed from 6)\n                    strategy_success_count = 0\n\n                    for strategy_num in range(1, 4):  # Changed from range(1, 7) to range(1, 4)\n                        logging.info(f\"Processing Strategy {strategy_num} for line {line_num}\")\n\n                        result = generate_perturbed_solution(\n                            api_manager, question, solution, exact_answer, strategy_num\n                        )\n\n                        # Store the perturbed solution\n                        obj[f'Wrong_Solution_Strategy_{strategy_num}'] = result['modified_solution']\n\n                        # Store enhanced metadata\n                        obj[f'Perturbation_Metadata_Strategy_{strategy_num}'] = {\n                            'error_type': f'strategy_{strategy_num}',\n                            'error_description': result['error_description'],\n                            'erroneous_part': result['erroneous_part'],\n                            'success': result['success'],\n                            'quality_metrics': result.get('quality_metrics', {'quality_score': 0, 'issues': ['No metrics available']}),\n                            'attempt_count': result.get('attempt', 1)\n                        }\n\n                        if result['success']:\n                            strategy_success_count += 1\n                            quality_scores.append(result.get('quality_metrics', {}).get('quality_score', 0))\n\n                        # Brief pause between strategies\n                        time.sleep(1)\n\n                    successful_perturbations += strategy_success_count\n\n                # Write result (proper JSONL format)\n                output_file.write(json.dumps(obj, ensure_ascii=False) + '\\n')\n                processed_count += 1\n\n                # Progress logging with quality metrics (changed frequency)\n                if processed_count % 10 == 0:  # Changed from every 5 to every 10 since we have fewer API calls\n                    stats = api_manager.get_usage_stats()\n                    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n                    logging.info(f\"Progress: {processed_count}/{total_objects}, Total Successful: {successful_perturbations}\")\n                    logging.info(f\"API Usage: {stats['total_used']}/{stats['total_available']} ({stats['percent_used']:.1f}%)\")\n                    logging.info(f\"Average Quality Score: {avg_quality:.1f}\")\n\n            except Exception as e:\n                logging.error(f\"Line {line_num}: Processing error: {e}\")\n                traceback.print_exc()\n                continue\n\n    # Final statistics with quality metrics\n    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n    print(f\"Enhanced solution perturbation completed!\")\n    print(f\"Total processed: {processed_count}\")\n    print(f\"Total successful perturbations: {successful_perturbations}\")\n    print(f\"Average successful perturbations per problem: {successful_perturbations/processed_count:.2f}\" if processed_count > 0 else \"No problems processed\")\n    print(f\"Average quality score: {avg_quality:.1f}\")\n    print(f\"Quality distribution: Min: {min(quality_scores) if quality_scores else 0:.1f}, Max: {max(quality_scores) if quality_scores else 0:.1f}\")\n    print(f\"Output saved to: {output_file_path}\")\n\n# ---------------------------\n# Main function\n# ---------------------------\n\ndef main():\n    \"\"\"Main function to generate enhanced perturbed solutions for English math problems.\"\"\"\n    # Initialize API keys (require keys loaded from Drive or environment; fail fast if missing)\n    api_keys = None\n\n    # 1) Try to use `keys` variable loaded earlier (from Drive)\n    if 'keys' in globals() and keys:\n        api_keys = [k.strip() for k in keys.split(',') if k.strip()]\n\n    # 2) Otherwise, try the environment variable GEMINI_API_KEYS\n    if not api_keys:\n        env_val = os.environ.get('GEMINI_API_KEYS', '').strip()\n        if env_val:\n            api_keys = [k.strip() for k in env_val.split(',') if k.strip()]\n\n    # Fail fast if no keys found\n    if not api_keys:\n        err_msg = (\n            \"ERROR: No API keys found. Please create a text file at \"\n            \"/content/drive/MyDrive/gemini_keys.txt containing a single line with keys \"\n            \"separated by commas (e.g. key1,key2,...) or set the GEMINI_API_KEYS environment variable.\"\n        )\n        print(err_msg)\n        raise SystemExit(err_msg)\n\n    print(f\"Initialized with {len(api_keys)} API keys\")\n\n    # Initialize enhanced API manager\n    api_manager = GeminiSolutionApiManager(\n        api_keys=api_keys,\n        calls_per_day=1000,\n        rate_limit_delay=4  # Reduced delay since we're making 3 calls per item instead of 6\n    )\n\n    # File paths (update as needed)\n    input_jsonl_path = \"/kaggle/input/nctb-dataset/English_Final_Corpus.jsonl\"\n    output_jsonl_path = \"/kaggle/working/Enhanced_Perturbed_English.jsonl\"\n\n    if not os.path.exists(input_jsonl_path):\n        print(f\"Input file not found: {input_jsonl_path}\")\n        return\n\n    print(\"Starting enhanced solution perturbation process...\")\n    print(\"Key improvements in this version:\")\n    print(\"- 3 Combined error strategies instead of 6 individual strategies\")\n    print(\"- Strategy 1: Step omission + Incorrect rules + Faulty reasoning\")\n    print(\"- Strategy 2: Overgeneralization + Logical fallacies\")\n    print(\"- Strategy 3: All five error types combined\")\n    print(\"- Error descriptions limited to exactly 2 meaningful lines\")\n    print(\"- Enhanced validation and quality metrics\")\n\n    try:\n        process_jsonl_file(api_manager, input_jsonl_path, output_jsonl_path)\n\n        # Final API usage summary\n        final_stats = api_manager.get_usage_stats()\n        print(\"\\n\" + \"=\"*50)\n        print(\"FINAL API USAGE SUMMARY\")\n        print(\"=\"*50)\n        print(f\"Total API calls made: {final_stats['total_used']}\")\n        print(f\"Total API calls available: {final_stats['total_available']}\")\n        print(f\"Utilization: {final_stats['percent_used']:.1f}%\")\n        print(\"Per-key usage:\")\n        for i, (key, usage) in enumerate(final_stats['per_key'].items(), 1):\n            key_preview = key[:8] + \"...\" if len(key) > 8 else key\n            print(f\"  Key {i} ({key_preview}): {usage} calls\")\n\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        logging.error(f\"Error processing file: {e}\")\n        traceback.print_exc()\n\n# ---------------------------\n# Utility Functions for Analysis\n# ---------------------------\n\ndef analyze_perturbation_results(jsonl_file_path):\n    \"\"\"\n    Analyze the results of the perturbation process to provide insights.\n    \"\"\"\n    print(f\"Analyzing perturbation results from: {jsonl_file_path}\")\n\n    if not os.path.exists(jsonl_file_path):\n        print(f\"File not found: {jsonl_file_path}\")\n        return\n\n    strategy_stats = {i: {'success': 0, 'total': 0, 'quality_scores': []} for i in range(1, 4)}  # Changed from range(1, 7)\n    total_problems = 0\n\n    with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            try:\n                obj = json.loads(line.strip())\n                total_problems += 1\n\n                for strategy_num in range(1, 4):  # Changed from range(1, 7)\n                    metadata_key = f'Perturbation_Metadata_Strategy_{strategy_num}'\n                    if metadata_key in obj:\n                        metadata = obj[metadata_key]\n                        strategy_stats[strategy_num]['total'] += 1\n\n                        if metadata.get('success', False):\n                            strategy_stats[strategy_num]['success'] += 1\n                            quality_score = metadata.get('quality_metrics', {}).get('quality_score', 0)\n                            strategy_stats[strategy_num]['quality_scores'].append(quality_score)\n\n            except Exception as e:\n                print(f\"Error analyzing line: {e}\")\n                continue\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"PERTURBATION ANALYSIS RESULTS\")\n    print(\"=\"*60)\n    print(f\"Total problems processed: {total_problems}\")\n\n    strategy_names = {\n        1: \"Step Omission + Incorrect Rules + Faulty Reasoning\",\n        2: \"Overgeneralization + Logical Fallacies\",\n        3: \"All Five Error Types Combined\"\n    }\n\n    for strategy_num in range(1, 4):  # Changed from range(1, 7)\n        stats = strategy_stats[strategy_num]\n        success_rate = (stats['success'] / stats['total'] * 100) if stats['total'] > 0 else 0\n        avg_quality = sum(stats['quality_scores']) / len(stats['quality_scores']) if stats['quality_scores'] else 0\n\n        print(f\"\\nStrategy {strategy_num}: {strategy_names[strategy_num]}\")\n        print(f\"  Success Rate: {success_rate:.1f}% ({stats['success']}/{stats['total']})\")\n        print(f\"  Average Quality Score: {avg_quality:.1f}\")\n        if stats['quality_scores']:\n            print(f\"  Quality Range: {min(stats['quality_scores']):.1f} - {max(stats['quality_scores']):.1f}\")\n\n    # Overall statistics\n    total_successful = sum(stats['success'] for stats in strategy_stats.values())\n    total_attempts = sum(stats['total'] for stats in strategy_stats.values())\n    overall_success_rate = (total_successful / total_attempts * 100) if total_attempts > 0 else 0\n\n    all_quality_scores = []\n    for stats in strategy_stats.values():\n        all_quality_scores.extend(stats['quality_scores'])\n\n    overall_avg_quality = sum(all_quality_scores) / len(all_quality_scores) if all_quality_scores else 0\n\n    print(f\"\\n\" + \"-\"*40)\n    print(\"OVERALL STATISTICS\")\n    print(\"-\"*40)\n    print(f\"Total perturbation attempts: {total_attempts}\")\n    print(f\"Total successful perturbations: {total_successful}\")\n    print(f\"Overall success rate: {overall_success_rate:.1f}%\")\n    print(f\"Overall average quality score: {overall_avg_quality:.1f}\")\n    print(f\"Average successful perturbations per problem: {total_successful/total_problems:.2f}\" if total_problems > 0 else \"No problems found\")\n\ndef sample_perturbations(jsonl_file_path, num_samples=3):\n    \"\"\"\n    Display sample perturbations for manual inspection.\n    \"\"\"\n    print(f\"\\nSampling {num_samples} perturbations from: {jsonl_file_path}\")\n\n    if not os.path.exists(jsonl_file_path):\n        print(f\"File not found: {jsonl_file_path}\")\n        return\n\n    samples_collected = 0\n\n    with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n        for line_num, line in enumerate(f, 1):\n            if samples_collected >= num_samples:\n                break\n\n            try:\n                obj = json.loads(line.strip())\n\n                # Check if this problem has successful perturbations\n                has_successful = False\n                for strategy_num in range(1, 4):  # Changed from range(1, 7)\n                    metadata_key = f'Perturbation_Metadata_Strategy_{strategy_num}'\n                    if (metadata_key in obj and\n                        obj[metadata_key].get('success', False)):\n                        has_successful = True\n                        break\n\n                if has_successful:\n                    samples_collected += 1\n                    question = get_field_case_insensitive(obj, 'Question', 'question', 'QuestionText')\n                    solution = get_field_case_insensitive(obj, 'Solution', 'solution')\n                    exact_answer = get_field_case_insensitive(obj, 'Exact Answer', 'ExactAnswer', 'exact answer', 'Answer')\n\n                    print(f\"\\n\" + \"=\"*80)\n                    print(f\"SAMPLE {samples_collected} (Line {line_num})\")\n                    print(\"=\"*80)\n                    print(f\"Question: {question[:200]}...\" if len(question) > 200 else f\"Question: {question}\")\n                    print(f\"Exact Answer: {exact_answer}\")\n\n                    # Show one successful perturbation\n                    for strategy_num in range(1, 4):  # Changed from range(1, 7)\n                        metadata_key = f'Perturbation_Metadata_Strategy_{strategy_num}'\n                        solution_key = f'Wrong_Solution_Strategy_{strategy_num}'\n\n                        if (metadata_key in obj and solution_key in obj and\n                            obj[metadata_key].get('success', False)):\n\n                            metadata = obj[metadata_key]\n                            perturbed_solution = obj[solution_key]\n                            quality_score = metadata.get('quality_metrics', {}).get('quality_score', 0)\n\n                            strategy_names = {\n                                1: \"Multi-Error (Step + Rule + Reasoning)\",\n                                2: \"Generalization + Logic Fallacies\",\n                                3: \"All Five Error Types\"\n                            }\n\n                            print(f\"\\nStrategy {strategy_num} - {strategy_names[strategy_num]} (Quality: {quality_score:.1f}):\")\n                            print(f\"Error Description: {metadata.get('error_description', 'N/A')}\")\n                            print(f\"Perturbed Solution: {perturbed_solution[:300]}...\" if len(perturbed_solution) > 300 else f\"Perturbed Solution: {perturbed_solution}\")\n                            break  # Show only one strategy per sample\n\n            except Exception as e:\n                print(f\"Error processing sample from line {line_num}: {e}\")\n                continue\n\nif __name__ == \"__main__\":\n    main()\n\n    # Optionally run analysis after processing\n    output_file = \"/kaggle/working/Enhanced_Perturbed_English.jsonl\"\n    if os.path.exists(output_file):\n        print(\"\\n\" + \"=\"*60)\n        print(\"Running post-processing analysis...\")\n        analyze_perturbation_results(output_file)\n        sample_perturbations(output_file, num_samples=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}